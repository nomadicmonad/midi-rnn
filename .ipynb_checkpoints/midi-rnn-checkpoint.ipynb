{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDI-RNN\n",
    "\n",
    "Tensorflow-based recurrent neural network for generating MIDI music.\n",
    "\n",
    "The input training data is a <a href=\"https://www.reddit.com/r/datasets/comments/3akhxy/the_largest_midi_collection_on_the_internet/\">huge</a> dataset of MIDI tunes scraped from the internet.\n",
    "\n",
    "The MIDI arrays are turned into numpy arrays that are fed into a tensorflow model that concatenates MIDI timing and note information. The network tries to predict the next note and its timing. Two LSTMs are used, along with a number of dense layers. The input data is also passed through a dot-product query layer, in some sense akin to attention heads.\n",
    "\n",
    "The network can then be made self-recursive to query a generated MIDI song, which is played by the pygame python module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "\n",
    "Lets begin by downloading and unzipping the MIDI zip file.\n",
    "\n",
    "Use <a href=\"https://mega.co.nz/#!Elg1TA7T!MXEZPzq9s9YObiUcMCoNQJmCbawZqzAkHzY4Ym6Gs_Q\">this</a> mega link to retrieve the file, unzip it, and move it to the working directory.\n",
    "\n",
    "<p style=\"font-weight:800\">WARNING</p>file size is over 1GB!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install numpy\n",
    "!pip intall mido\n",
    "!pip install tensorflow==1.8.0\n",
    "!pip install tensorboard\n",
    "!pip install tqdm\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import mido\n",
    "from mido import Message, MidiFile, MidiTrack, MAX_PITCHWHEEL\n",
    "import glob\n",
    "import copy\n",
    "from subprocess import Popen\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from IPython.display import IFrame\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_midi_track(midi_notes,midi_timing):\n",
    "    '''\n",
    "    Utility function to turn RNN outputs into mido MIDI track.\n",
    "    \n",
    "    Inputs\n",
    "    \n",
    "    midi_notes: numpy array of note data, from calling Tensorflow forward pass\n",
    "    midi_timing: numpy array of note timing, from calling Tensorflow forward pass\n",
    "    \n",
    "    Function\n",
    "    \n",
    "    Parses correct MIDI relative timing, and adds notes and their timing to mido track file,\n",
    "    subsequently saving the track as a .mid MIDI file.\n",
    "    \n",
    "    Outputs\n",
    "    \n",
    "    None\n",
    "    '''\n",
    "    outfile = MidiFile(type=0)\n",
    "    track = MidiTrack()\n",
    "    outfile.tracks.append(track)\n",
    "    aggregator = []\n",
    "    aggregator_times = []\n",
    "    for index,msg_array in enumerate(midi_notes):\n",
    "        msg_array = np.packbits(msg_array,axis=0)\n",
    "        if (msg_array[0] > 0):\n",
    "            for x in range(1,1+min(3,msg_array[4])):\n",
    "                aggregator.append(msg_array[x])\n",
    "                aggregator_times.append(midi_timing[index])\n",
    "        else:\n",
    "                if len(aggregator) > 0:\n",
    "                    try:\n",
    "                        msg = Message.from_bytes(aggregator)\n",
    "                        msg.time = max(0,int(np.round(np.mean(np.asarray(aggregator_times))*10)))\n",
    "                        track.append(msg)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                    aggregator = []\n",
    "                    aggregator_times = []\n",
    "                try:\n",
    "                    msg = Message.from_bytes(msg_array[1:1+min(3,msg_array[4])])\n",
    "                    msg.time = max(0,int(np.round(np.mean(midi_timing[index])*10)))\n",
    "                    track.append(msg)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "    try:\n",
    "        outfile.save('generated_midi.mid')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "def load_midi_file(filename):\n",
    "    '''\n",
    "    Utility function for loading midi file as numpy data and timing arrays\n",
    "    \n",
    "    Inputs\n",
    "    \n",
    "    filename: string denoting file location\n",
    "    \n",
    "    Function:\n",
    "    \n",
    "    midi note data is stored as a (-1,5) array.\n",
    "    This is because some MIDI commands are longer than one byte.\n",
    "    The first scalar (cur[0]) denotes whether this MIDI\n",
    "    command is longer than one byte.\n",
    "    The final scalar (cur[4]) denotes the byte order.\n",
    "    The scalars in between store the byte values.\n",
    "    \n",
    "    Finally, the note data is unpacked into bits, producing a size-40 array\n",
    "    (1 byte * 5 = 8 bits * 5 = 40)\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    midi_data: numpy data array indicating the midi note and velocity as well as which\n",
    "    midi bytes are part of the same note\n",
    "    midi_time: numpy timing data array for the midi notes\n",
    "    \n",
    "    '''\n",
    "    midi_file = mido.MidiFile(filename)\n",
    "    midi = [msg for msg in midi_file]\n",
    "    bytes_num = np.max(np.asarray([len(x.bytes()) for x in midi]))\n",
    "    midi_data = []\n",
    "    midi_time = []\n",
    "    for x in midi:\n",
    "        cur = np.zeros((5))\n",
    "        for ind,byte in enumerate(x.bytes()):\n",
    "            if (len(x.bytes())>3):\n",
    "                cur[0] = 1\n",
    "            cur[1 + (ind)%3] = byte\n",
    "            if ((ind+1) % 3 ==0 or ind + 1 == len(x.bytes())):\n",
    "                cur[4] = (ind % 3) + 1\n",
    "                midi_data.append(cur)\n",
    "                midi_time.append(x.time*midi_file.ticks_per_beat*2)\n",
    "                cur = np.zeros((5))\n",
    "    midi_data = np.asarray(midi_data,dtype=np.dtype('B'))\n",
    "    midi_time = np.asarray(midi_time).astype(np.float32)\n",
    "    midi_data = np.unpackbits(midi_data,axis=1).astype(np.int32)\n",
    "    return midi_data, midi_time\n",
    "            \n",
    "def is_midi_0(filename):\n",
    "    '''\n",
    "    Utility function for testing if .mid file is type 0 (delta-time) and loads correctly\n",
    "    '''\n",
    "    try:\n",
    "        return mido.MidiFile(filename).type == 0\n",
    "    except Exception as e:\n",
    "        return False\n",
    "    \n",
    "def extract_samples(midis_array=None,midi_times_array=None,p=0, seq_length=25, current_file=0):\n",
    "    '''\n",
    "    Utility function for retrieving next RNN input and target arrays,\n",
    "    as well as applying basic augmentation through addition of noise,\n",
    "    and setting random time-points to zero.\n",
    "    '''\n",
    "    input_midi_val = midis_array[current_file][p:p+seq_length].reshape(seq_length,5*8).astype(np.float32)\n",
    "    input_time_val = midi_times_array[current_file][p:p+seq_length].reshape(seq_length,1).astype(np.float32)\n",
    "    target_midi_val = midis_array[current_file][p + 1: p+seq_length + 1].reshape(seq_length,5*8)\n",
    "    target_time_val = midi_times_array[current_file][p + 1: p+seq_length + 1].reshape(seq_length,1)\n",
    "    input_midi_val += np.random.normal(scale=0.1,size=seq_length*5*8).reshape(seq_length,5*8)\n",
    "    input_time_val += np.random.normal(scale=0.1,size=seq_length).reshape(seq_length,1)\n",
    "    input_midi_val = np.maximum(0,input_midi_val)\n",
    "    input_time_val = np.maximum(0,input_time_val)\n",
    "\n",
    "    random_int = int(np.random.randint(seq_length,size=1))\n",
    "    input_midi_val[random_int] = np.zeros((1,5*8))\n",
    "    input_time_val[random_int] = np.zeros((1,1))\n",
    "    return input_midi_val, input_time_val, target_midi_val, target_time_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "We will use glob to find the MIDI file-paths from the dataset we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "file_list = sorted(glob.glob(cwd + \"/**/**/*.mid\",recursive=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters & Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "tf.set_random_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "hidden_size = 500\n",
    "dense_size = 100\n",
    "seq_length = int(25)\n",
    "dropout_rate = 0.95\n",
    "learning_rate = 1e-4 # Adam Learning Rate\n",
    "iterations = 1000 # Iterations per training run\n",
    "max_midi_files_in_memory = 500 # Number of MIDI files stored in memory\n",
    "loading_rate = 0.99 # Rate of replacing MIDi files in memory with new samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders Etc.\n",
    "\n",
    "Here we set up placeholder nodes for the TF graph: RNN inputs and targets, LSTM states, the\n",
    "global step of the optimiser,the dropout rate, and RNN sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "input_midi     = tf.placeholder(shape=[seq_length,5*8], dtype=tf.float32, name=\"inputs\")\n",
    "input_time     = tf.placeholder(shape=[seq_length,1], dtype=tf.float32, name=\"inputs_time\")\n",
    "targets_midi     = tf.placeholder(shape=[seq_length,5*8], dtype=tf.float32, name=\"targets\")\n",
    "targets_time     = tf.placeholder(shape=[seq_length,1], dtype=tf.float32, name=\"targets_time\")\n",
    "\n",
    "ic0_c = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name=\"statec0c\")\n",
    "ic0_h = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name=\"statec0h\")\n",
    "ic1_c = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name=\"statec1c\")\n",
    "ic1_h = tf.placeholder(shape=[1, hidden_size], dtype=tf.float32, name=\"statec1h\")\n",
    "\n",
    "rate_ph = tf.placeholder(shape=[],dtype=tf.float32,name=\"dropout\")\n",
    "seq_length_ph = tf.placeholder(shape=[1], dtype=tf.int32, name=\"seqlength\")\n",
    "\n",
    "initial_state_c0 = tf.nn.rnn_cell.LSTMStateTuple(ic0_c, ic0_h)\n",
    "initial_state_c1 = tf.nn.rnn_cell.LSTMStateTuple(ic1_c, ic1_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Graph\n",
    "\n",
    "This is the Tensorflow graph for the neural network.\n",
    "The MIDI timing and note values are concatenated into inputs0.\n",
    "This layer is passed through an l2-normalised dot-product query-mapping,\n",
    "and is used to gate another dense layer that uses the same inputs.\n",
    "The gated dense layer is then fed into the first LSTM.\n",
    "The output of this LSTM passes through a further dense layer, before\n",
    "passing through the second LSTM. The output of this final LSTM is passed through\n",
    "yet another dense layer, which forks into two dense layers, that respectively feed\n",
    "into dense layers predicting the target MIDI note and timing values for the next timestep.\n",
    "The forking is used to allow the network capacity to split the hidden representation into\n",
    "timing and note distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"RNN\") as scope:\n",
    "\n",
    "        rnn1f = rnn.LSTMCell(hidden_size, reuse=tf.AUTO_REUSE,name=\"lstm1\", initializer = tf.random_normal_initializer(stddev=0.01))\n",
    "        rnn2f = rnn.LSTMCell(hidden_size, reuse=tf.AUTO_REUSE,name=\"lstm2\", initializer = tf.random_normal_initializer(stddev=0.01))\n",
    "\n",
    "        zero_state1 = rnn1f.zero_state(1,tf.float32)\n",
    "        zero_state2 = rnn2f.zero_state(1,tf.float32)\n",
    "\n",
    "        inputs0 = tf.concat([input_midi,input_time],axis=1)\n",
    "        query_table = tf.Variable(np.random.normal(scale=0.01,size=(hidden_size,41)),dtype=tf.float32,name=\"query\")\n",
    "        query_norm = tf.transpose(tf.nn.l2_normalize(1e-7+query_table,axis=1))\n",
    "        multiplier = tf.Variable(1.0,dtype=tf.float32,name=\"multiplier\")\n",
    "        gating = tf.nn.tanh(multiplier*tf.matmul(tf.nn.l2_normalize(inputs0 + 1e-7,axis=1),query_norm))\n",
    "        inputs1_gated = gating*tf.layers.dense(inputs0, name=\"inputs1_gated\",units=hidden_size,activation=tf.nn.leaky_relu,kernel_initializer = tf.random_normal_initializer(stddev=0.01),reuse=tf.AUTO_REUSE)\n",
    "        outputs1f, states1f = rnn.static_rnn(rnn1f, tf.split(inputs1_gated,seq_length), initial_state=initial_state_c0, dtype=tf.float32,sequence_length=seq_length_ph)\n",
    "        mid2b = tf.layers.dense(tf.concat(outputs1f,axis=0), name=\"mid2b\",units=hidden_size,activation=tf.nn.leaky_relu,kernel_initializer = tf.random_normal_initializer(stddev=0.01),reuse=tf.AUTO_REUSE)\n",
    "        outputs2f, states2f = rnn.static_rnn(rnn2f, tf.split(tf.nn.dropout(mid2b,rate_ph),seq_length),initial_state=initial_state_c1, dtype=tf.float32,sequence_length=seq_length_ph)\n",
    "        output_fork = tf.layers.dense(tf.concat(outputs2f,axis=0), name=\"output_fork\",units=hidden_size,activation=tf.nn.leaky_relu,kernel_initializer = tf.random_normal_initializer(stddev=0.01),reuse=tf.AUTO_REUSE)\n",
    "        pre_midi_output = tf.layers.dense(output_fork, units =  dense_size, name='pre_midi_output', activation=tf.nn.leaky_relu, kernel_initializer = tf.random_normal_initializer(stddev=0.01),reuse=tf.AUTO_REUSE)\n",
    "        midi_output = tf.layers.dense(pre_midi_output, units =  5*8, name='midi_output',activation=None, kernel_initializer = tf.random_normal_initializer(stddev=0.01),reuse=tf.AUTO_REUSE)\n",
    "        pre_midi_time = tf.layers.dense(output_fork, units =  dense_size, name='pre_midi_time',activation=tf.nn.leaky_relu, kernel_initializer = tf.random_normal_initializer(stddev=0.01),reuse=tf.AUTO_REUSE)\n",
    "        midi_time = tf.layers.dense(pre_midi_time, units =  1, name='midi_time',activation=tf.nn.leaky_relu,kernel_initializer = tf.random_normal_initializer(stddev=0.1),reuse=tf.AUTO_REUSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN outputs, Loss function, Optimiser\n",
    "We take the sigmoid of the midi output predictions to constrain the value between 0 and 1.\n",
    "We apply both MSE loss and cosine loss to make the network learn the bit-representations of MIDI commands,\n",
    "as well as absolute values. For the timing loss, we use purely RMSE.\n",
    "\n",
    "We apply gradient clipping to stabilise learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "midi_output = tf.nn.sigmoid(midi_output)\n",
    "loss_mse = tf.reduce_mean((midi_output- targets_midi)**2)\n",
    "loss_mse = tf.where(tf.is_nan(loss_mse), tf.zeros_like(loss_mse), loss_mse)\n",
    "loss_cosine = tf.reduce_mean(tf.losses.cosine_distance(axis=1,labels=tf.nn.l2_normalize(1e-7+midi_output),\\\n",
    "                                                       predictions=tf.nn.l2_normalize(1e-7+targets_midi))**2)\n",
    "\n",
    "loss_time =  tf.sqrt(1e-7 + tf.reduce_mean(((midi_time- targets_time))**2))\n",
    "loss_time = tf.where(tf.is_nan(loss_time), tf.zeros_like(loss_time), loss_time)\n",
    "\n",
    "loss = (loss_mse + 0.001*loss_cosine + loss_time*0.001)\n",
    "\n",
    "tf.summary.scalar('loss/loss',loss)\n",
    "tf.summary.scalar('loss/mse',loss_mse)\n",
    "tf.summary.scalar('loss/mse_time',loss_time)\n",
    "tf.summary.histogram('outputs/outputs1',midi_output)\n",
    "tf.summary.histogram('targets/targets',targets_midi)\n",
    "tf.summary.histogram('outputs/time',midi_time)\n",
    "tf.summary.histogram('targets/time',targets_time)\n",
    "\n",
    "# Minimizer\n",
    "minimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Gradient clipping\n",
    "grad_clipping = tf.constant(5.0, name=\"grad_clipping\")\n",
    "clipped_grads_and_vars = []\n",
    "for index,the_tuple in enumerate(minimizer.compute_gradients(loss,var_list=tf.trainable_variables())):\n",
    "    grad, var = the_tuple[0], the_tuple[1]\n",
    "    clipped_grad = tf.clip_by_value(grad, -grad_clipping, grad_clipping)\n",
    "    clipped_grad = tf.where(tf.is_nan(clipped_grad), tf.zeros_like(clipped_grad), clipped_grad)\n",
    "    clipped_grads_and_vars.append((clipped_grad, var))\n",
    "\n",
    "updates = minimizer.apply_gradients(clipped_grads_and_vars,global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Session, Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "init2 = tf.initialize_all_variables()\n",
    "train_writer = tf.summary.FileWriter(\"./rnn_audio/summary_rnn\", sess.graph)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "restore = False\n",
    "if (not restore):\n",
    "    sess.run(init)\n",
    "    sess.run(init2)\n",
    "else:\n",
    "    checkpoint = 0 # Add last checkpoint file number here\n",
    "    saver.restore(sess,\"./rnn_audio/rnn_audio-{}\".format(checkpoint))\n",
    "\n",
    "midis_array = []\n",
    "midi_times_array = []\n",
    "first_midi_array, first_midi_times_array = load_midi_file(file_list[0])\n",
    "midis_array.append(first_midi_array)\n",
    "midi_times_array.append(first_midi_times_array)\n",
    "p_list = [0]\n",
    "rnn_state1_list = [copy.deepcopy(sess.run(zero_state1))]\n",
    "rnn_state2_list = [copy.deepcopy(sess.run(zero_state2))]\n",
    "file_counter = len(midis_array)-1\n",
    "current_file = 0\n",
    "last_iteration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tensorboard\n",
    "\n",
    "We use subprocess.Popen for an async call to tensorboard, then display the localhost website in an iFrame.\n",
    "You can subsequently call the cell after the training-code to terminate the tensorboard process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = Popen(['tensorboard','--logdir=./rnn_audio/summary_rnn']) # something long running\n",
    "time.sleep(10) # Sleep while tensorboard setting up.\n",
    "IFrame(src=\"http://localhost:6006/\", width='100%', height='500px')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "This is the main training loop of the notebook. You can re-run this to keep training the network.\n",
    "There is a sample-pool of MIDI files that is randomly refreshed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for iteration in tqdm(range(last_iteration,last_iteration+iterations)):\n",
    "    # Initialize\n",
    "    current_file = int(np.random.randint(len(midis_array),size=1))\n",
    "    while (p_list[current_file] + seq_length + 1 > midis_array[current_file].shape[0]):\n",
    "        # Reset current file tracking   \n",
    "        p_list[current_file] = 0\n",
    "        rnn_state1_list[current_file] = copy.deepcopy(sess.run(zero_state1))\n",
    "        rnn_state2_list[current_file] = copy.deepcopy(sess.run(zero_state2))\n",
    "        current_file = int(np.random.randint(len(midis_array),size=1))\n",
    "        # Transition to loading new file\n",
    "        if (np.random.uniform() > loading_rate):\n",
    "            file_loaded = False\n",
    "            while (not file_loaded or (file_choice in loaded_files)):\n",
    "                try:\n",
    "                    file_choice = int(np.random.randint(len(file_list),size=1))\n",
    "                    if (is_midi_0(file_list[file_choice])):\n",
    "                        midi_file = mido.MidiFile(file_list[file_choice])\n",
    "                        file_loaded = True\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            p_list.append(0)\n",
    "            rnn_state1_list.append(copy.deepcopy(sess.run(zero_state1)))\n",
    "            rnn_state2_list.append(copy.deepcopy(sess.run(zero_state2)))\n",
    "            if (len(midis_array) > max_midi_files_in_memory):\n",
    "                midis_array = midis_array[1:max_midi_files_in_memory]\n",
    "                midi_times_array = midi_times_array[1:max_midi_files_in_memory]\n",
    "                p_list = p_list[1:max_midi_files_in_memory]\n",
    "                rnn_state1_list = rnn_state1_list[1:max_midi_files_in_memory]\n",
    "                rnn_state2_list = rnn_state2_list[1:max_midi_files_in_memory]\n",
    "                loaded_files = loaded_files[1:max_midi_files_in_memory]\n",
    "            current_file = len(midis_array)-1\n",
    "            \n",
    "    p = p_list[current_file]\n",
    "    input_midi_val, input_time_val, target_midi_val, target_time_val = \\\n",
    "    extract_samples(midis_array=midis_array,midi_times_array=midi_times_array,p=p, seq_length=seq_length, current_file=current_file)\n",
    "\n",
    "    states1f_val,states2f_val,loss_val,_,summary = sess.run([states1f,states2f,loss,updates,merged],\n",
    "                                      feed_dict={input_midi : input_midi_val,\n",
    "                                                 input_time : input_time_val,\n",
    "                                                 ic1_c: rnn_state2_list[current_file].c,\n",
    "                                                 ic1_h: rnn_state2_list[current_file].h,\n",
    "                                                 ic0_c: rnn_state1_list[current_file].c,\n",
    "                                                 ic0_h: rnn_state1_list[current_file].h,\n",
    "                                                 rate_ph: dropout_rate,\n",
    "                                                 targets_midi: target_midi_val,\n",
    "                                                 targets_time : target_time_val,\n",
    "                                                 seq_length_ph:np.ones((1))*seq_length\n",
    "                                                 })\n",
    "    rnn_state1_list[current_file] = states1f_val\n",
    "    rnn_state2_list[current_file] = states2f_val\n",
    "\n",
    "    if (iteration % 200 == 0):\n",
    "        train_writer.add_summary(summary, iteration)\n",
    "        print('iter: {}, p: {}, loss: {}'.format(iteration, p, loss_val))\n",
    "    p_list[current_file] += seq_length\n",
    "\n",
    "saver.save(sess,\"./rnn_audio/rnn_audio\",global_step=global_step)\n",
    "last_iteration += iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Close Tensorboard\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate RNN MIDI Music\n",
    "\n",
    "Here we feed the predictions of the RNN model into itself, to generate\n",
    "novel MIDI compositions, which can then be played later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_state1 = copy.deepcopy(sess.run(zero_state1))\n",
    "rnn_state2 = copy.deepcopy(sess.run(zero_state2))\n",
    "midi_data_vals = []\n",
    "midi_time_vals = []\n",
    "input_midi_val = midis_array[current_file][0:seq_length].reshape(seq_length,5*8)\n",
    "input_time_val = midi_times_array[current_file][0:seq_length].reshape(seq_length,1)\n",
    "\n",
    "midi_data_vals.append(np.around(input_midi_val[0]).astype(np.int32))\n",
    "midi_time_vals.append(np.around(input_time_val[0]).astype(np.int32))\n",
    "\n",
    "generated_midi_length = 2500\n",
    "        \n",
    "print('Sampling a generated MIDI file')\n",
    "for t in tqdm(range(generated_midi_length)):\n",
    "    midi_data_current,midi_time_current,rnn_state1,rnn_state2 = \\\n",
    "                    sess.run([midi_output,midi_time,states1f,states2f],\n",
    "                             feed_dict={input_midi: input_midi_val,\n",
    "                                                     input_time : input_time_val,\n",
    "                                                     ic1_c: rnn_state2.c,\n",
    "                                                     ic1_h: rnn_state2.h,\n",
    "                                                     ic0_c: rnn_state1.c,\n",
    "                                                     ic0_h: rnn_state1.h,\n",
    "                                                     rate_ph: 1.0,\n",
    "                                                     seq_length_ph: np.ones((1))\n",
    "                                                     })\n",
    "            \n",
    "    input_midi_val[0] = np.maximum(0,np.asarray(np.nan_to_num(midi_data_current[0].reshape(1,5*8))))\n",
    "    input_time_val[0] = np.maximum(0,np.asarray(np.nan_to_num(midi_time_current[0].reshape(1,1))))\n",
    "    midi_data_vals.append(np.maximum(0,np.around(midi_data_current[0]).astype(np.int32)))\n",
    "    midi_time_vals.append(np.maximum(0,np.around(midi_time_current[0]).astype(np.float32)))\n",
    "\n",
    "create_midi_track(midi_data_vals,midi_time_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play MIDI tune\n",
    "\n",
    "MIDI pygame example from <a href=\"https://www.daniweb.com/programming/software-development/code/216979/embed-and-play-midi-music-in-your-code-python\">here<a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def play_music(music_file):\n",
    "    \"\"\"\n",
    "    stream music with mixer.music module in blocking manner\n",
    "    this will stream the sound from disk while playing\n",
    "    \"\"\"\n",
    "    clock = pygame.time.Clock()\n",
    "    try:\n",
    "        pygame.mixer.music.load(music_file)\n",
    "    except pygame.error as e:\n",
    "        print(e)\n",
    "        return\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        # check if playback has finished\n",
    "        clock.tick(30)\n",
    "freq = 44100    # audio CD quality\n",
    "bitsize = -16   # unsigned 16 bit\n",
    "channels = 2    # 1 is mono, 2 is stereo\n",
    "buffer = 1024    # number of samples\n",
    "pygame.mixer.init(freq, bitsize, channels, buffer)\n",
    "# optional volume 0 to 1.0\n",
    "pygame.mixer.music.set_volume(0.3)\n",
    "try:\n",
    "    # use the midi file you just saved\n",
    "    play_sample = True\n",
    "    sample_path = os.path.join(os.getcwd(),\"sample.mid\")\n",
    "    generated_path = os.path.join(os.getcwd(),\"generated_midi.mid\")\n",
    "    play_music(sample_path if play_sample else generated_path)\n",
    "except KeyboardInterrupt:\n",
    "    # if user hits Ctrl/C then exit\n",
    "    # (works only in console mode)\n",
    "    pygame.mixer.music.fadeout(1000)\n",
    "    pygame.mixer.music.stop()\n",
    "    raise SystemExit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
